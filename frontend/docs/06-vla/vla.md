---
sidebar_position: 1
---

# Vision-Language-Action (VLA) Models – The Cognitive Layer

This module covers Vision-Language-Action (VLA) models, including:

- Whisper → LLM → Action pipeline
- Prompting LLMs to generate ROS 2 action sequences
- OpenAI Realtime API + function calling for robots
- Building a voice-commanded autonomous humanoid
