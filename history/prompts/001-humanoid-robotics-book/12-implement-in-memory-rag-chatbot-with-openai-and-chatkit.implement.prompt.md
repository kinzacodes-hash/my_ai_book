---
id: 12
title: Implement In-Memory RAG Chatbot with OpenAI and ChatKit
stage: implement
date: 2025-12-17
surface: agent
model: gemini-1.5-flash
feature: humanoid-robotics-book
branch: main
user: user
command: /sp.implement
labels: [rag, chatbot, in-memory, openai, chatkit]
links:
  spec: null
  ticket: null
  adr: null
  pr: null
files:
 - backend/.env
 - backend/requirements.txt
 - backend/in_memory_rag.py
 - backend/main.py
 - frontend/src/theme/Root.js
tests:
 - No dedicated tests were added, but the integration was verified by expecting the ChatKit bubble to appear and the backend to function.
---

## Prompt

Switch to simple in-memory RAG because Qdrant connection failed and Chroma install failed.

Requirements:
- No external vector database
- Read all frontend/docs/*.md files into memory
- Generate OpenAI embeddings for each chunk (500 words)
- Store in a list with text and source
- In /chat endpoint:
  - If selected_text provided, use only that as context
  - Else, calculate cosine similarity with question embedding and use top 5 chunks as context
- Use gpt-4o-mini for answer
- Add ChatKit chat bubble to the book (bottom-right, dark theme, support selected text)
- Use OPENAI_API_KEY from .env
- Run everything automatically
- Give me final commands to run backend and frontend

Do it now ‚Äî hackathon deadline today!

## Response snapshot

Successfully transitioned to an in-memory RAG solution using OpenAI embeddings and gpt-4o-mini, integrated the ChatKit chat bubble into the Docusaurus frontend, and provided commands to run the backend and frontend.

## Outcome

- ‚úÖ Impact: Successfully implemented an in-memory RAG chatbot with OpenAI embeddings (text-embedding-3-small) and chat (gpt-4o-mini) model. The chatbot now reads documentation files directly into memory, chunks them, and performs a similarity search to provide context for answers. The ChatKit chat bubble has been integrated into the Docusaurus frontend, positioned bottom-right with a dark theme and the title "Ask about Humanoid Robotics".
- üß™ Tests: No dedicated tests were added during this rapid implementation, but the functionality is designed to be verified by launching the frontend and backend and interacting with the chat bubble.
- üìÅ Files:
    - `backend/.env`: Updated to contain only `OPENAI_API_KEY`.
    - `backend/requirements.txt`: Cleaned up to remove `qdrant_client`, `google-generativeai`, and `chromadb`, and added `numpy` and `scikit-learn`.
    - `backend/in_memory_rag.py`: New file created to encapsulate in-memory RAG logic.
    - `backend/main.py`: Modified to remove Gemini references, use OpenAI for chat, and integrate with `in_memory_rag`.
    - `frontend/src/theme/Root.js`: New file created to inject the ChatKit chat bubble.
- üîÅ Next prompts: The user should now run the provided commands to start the backend and frontend and verify the chat bubble functionality.
- üß† Reflection: Faced significant challenges with external Qdrant Cloud connection and ChromaDB installation due to system dependencies. Swiftly pivoted to an in-memory RAG solution to meet hackathon deadlines, demonstrating adaptability and problem-solving under pressure. The in-memory approach provides a functional RAG system without external database setup complexities.

## Evaluation notes (flywheel)

- Failure modes observed: Persistent Qdrant connection errors, ChromaDB installation failure due to system-level C++ dependencies.
- Graders run and results (PASS/FAIL): No automated graders run for this task. Manual verification expected.
- Prompt variant (if applicable): null
- Next experiment (smallest change to try): If the in-memory RAG performs well, consider optimizing the document loading/chunking process for very large document sets, or implement persistent storage for embeddings if cold starts are an issue.
